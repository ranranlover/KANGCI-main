import torch
import torch.nn as nn
import numpy as np
from sklearn.model_selection import ParameterGrid
from ComputeROC import compute_roc, compute_auprc
from src.efficient_kan import KAN
# from src.efficient_kan.kan02inter import KAN
from tool import dream_read_label
import scipy.io as sio
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from sklearn import metrics
import os
from sklearn.preprocessing import MinMaxScaler, StandardScaler
import pandas as pd

device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')
def compute_roc_1219(gc_label, gc_predict, draw):
    gc_label = gc_label.flatten().astype(float)
    gc_predict = gc_predict.flatten().astype(float)
    if draw:
        score = draw_roc_curve(gc_label, gc_predict / gc_predict.max())
        aupr = metrics.average_precision_score(gc_label, gc_predict / gc_predict.max())
    else:
        score = metrics.roc_auc_score(gc_label, gc_predict / gc_predict.max())
        aupr = metrics.average_precision_score(gc_label, gc_predict / gc_predict.max())
    return score, aupr


def draw_roc_curve(label, predict):
    FPR, TPR, P = metrics.roc_curve(label, predict)
    plt.plot(FPR, TPR, 'b*-', label='roc')
    plt.plot([0, 1], [0, 1], 'r--', label="45Â°")
    plt.legend()
    plt.xlabel('FPR')
    plt.ylabel('TPR')
    plt.show()
    AUC_score = metrics.auc(FPR, TPR)
    return AUC_score
class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MLP, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, x):
        return self.net(x)

def compute_gradient_gc_smooth_universal_v3_1220_103v1_san(
    model,
    input_seq_local,
    create_graph=True,
    lag_agg_local='mean',
    freq_denoise=True,
    cutoff_ratio=0.2,
    lambda_gc=1.0,
    tau=0.1,
    use_first_order_prior=True,
    tau_prior=0.05,
    beta_prior=5.0
):
    device = input_seq_local.device
    T, P = input_seq_local.shape[1], input_seq_local.shape[2]

    inp = input_seq_local.detach().clone().requires_grad_(True)

    # ---- å‰å‘ ----
    outs = model(inp).squeeze(0)  # (T, P)

    GCs_local = torch.zeros((P, P), device=device)
    gc_l1_loss = 0.0


    for j in range(P):
        s_j = outs[:, j].sum()

        if create_graph:
            retain = True
        else:
            retain = (j < P - 1)

        grads = torch.autograd.grad(
            s_j,
            inp,
            create_graph=create_graph,
            retain_graph=retain
        )[0]  # (1, T, P)

        grads_abs = grads.abs()

        # ---- å¹³æ»‘ ----
        if freq_denoise:
            g_fft = torch.fft.rfft(grads_abs.transpose(1, 2), dim=-1)
            n_freq = g_fft.shape[-1]
            cutoff = max(1, int(n_freq * cutoff_ratio))
            mask = torch.zeros_like(g_fft)
            mask[..., :cutoff] = 1.0
            g_fft = g_fft * mask
            grads_abs = torch.fft.irfft(
                g_fft,
                n=grads_abs.shape[1],
                dim=-1
            ).transpose(1, 2).abs()
        else:
            pass
            print('no use fre')


        # ---- lag èšåˆ ----
        g = grads_abs.squeeze(0)  # (T, P)

        if lag_agg_local == 'mean':
            gc_row = g.mean(dim=0)

        elif lag_agg_local == 'max':
            gc_row = g.max(dim=0)[0]

        elif lag_agg_local == 'rms':
            gc_row = torch.sqrt((g ** 2).mean(dim=0) + 1e-12)

        elif lag_agg_local == 'lp':
            p = 2.0
            gc_row = (g.pow(p).mean(dim=0) + 1e-12).pow(1.0 / p)

        elif lag_agg_local == 'softmax':
            w = torch.softmax(g / tau, dim=0)
            gc_row = (w * g).sum(dim=0)

        elif lag_agg_local == 'quantile':
            gc_row = g.quantile(0.9, dim=0)

        else:
            raise ValueError(f"Unknown lag_agg_local: {lag_agg_local}")


        if use_first_order_prior:
            gc_row_1st = torch.sqrt((g ** 2).mean(dim=0) + 1e-12)

            # soft priorï¼Œä¸åä¼ 
            gc_prior = torch.sigmoid(
                (gc_row_1st.detach() - tau_prior) * beta_prior
            )

            gc_l1_loss = gc_l1_loss + (gc_prior * gc_row.abs()).sum()
        else:
            gc_l1_loss = gc_l1_loss + gc_row.abs().sum()
            print('no use prior')

        GCs_local[j, :] = gc_row

        del grads, grads_abs

    L_gc = lambda_gc * gc_l1_loss

    return GCs_local, L_gc

def load_causaltime_seq(data_path, device, scaler_type='minmax', n_max=None):
    """
    åŠ è½½ CausalTime æ•°æ®ï¼Œä½¿å…¶æ ¼å¼ä¸ DREAM4 å…¼å®¹ï¼š
        input_seq:  (1, T-1, P)
        target_seq: (T-1, P)
        reversed_input_seq:  (1, T-1, P)
        reversed_target_seq: (T-1, P)
        mask: (P, P)
        X_raw: (T, P)  æ ‡å‡†åŒ–åçš„åŸå§‹åºåˆ—

    å‚æ•°ï¼š
        data_path: åŒ…å« data.npy å’Œ graph.npy çš„æ–‡ä»¶å¤¹
        device:    torch device
        scaler_type: æ ‡å‡†åŒ–æ–¹å¼
        n_max:     å¯é€‰ï¼Œé™åˆ¶å˜é‡æ•°ï¼ˆPï¼‰
    """

    # ------------------------------
    # 1. è¯»å– data.npy (T, P)
    # ------------------------------
    data_file = os.path.join(data_path, "gen_data.npy")
    print(f"Loading data from {data_file}")
    if not os.path.exists(data_file):
        raise FileNotFoundError(f"data.npy not found in {data_path}")

    X = np.load(data_file)  # (T, P)
    if X.ndim == 3:
        # å‡è®¾å½¢çŠ¶æ˜¯ (N_samples, T_per_sample, P) = (480, 40, 40)
        N_samples, T_per_sample, P_original = X.shape
        T_total = N_samples * T_per_sample
        print(f"Loaded 3D data with shape {X.shape}. Flattening to 2D shape ({T_total}, {P_original}).")

        # å°†å‰ä¸¤ä¸ªç»´åº¦ (480 * 40 = 19200) åˆå¹¶ä¸ºæ€»æ—¶é—´æ­¥ T_total
        X = X.reshape(T_total, P_original)
        scaler = StandardScaler()
        X = scaler.fit_transform(X)

    elif X.ndim != 2:
        # å¦‚æœä¸æ˜¯ 2D ä¹Ÿä¸æ˜¯ 3Dï¼Œåˆ™æŠ›å‡ºé”™è¯¯
        raise ValueError(f"Data array must be 2D (T, P) or 3D (N_samples, T_per_sample, P). Got shape {X.shape}")
    # ------------------------------
    # 2. å¯é€‰ï¼šè£å‰ªç»´åº¦ P
    # ------------------------------
    # if n_max is not None and X.shape[1] > n_max:
    #     X = X[:, :n_max]
    P_original = X.shape[1]
    if n_max is not None and P_original > n_max:
        # P_original = 40, n_max = 20
        print(f"Truncating features from P={P_original} to P={n_max} to match graph dimensions.")
        X = X[:, :n_max]
    # ------------------------------
    # 3. æ’å€¼ç¼ºå¤±å€¼
    # ------------------------------
    # mask_nan = np.isnan(X)
    # X_masked = np.ma.masked_array(X, mask_nan)
    # X_interp = pd.DataFrame(X_masked).interpolate(limit_direction='both').values
    # X = np.nan_to_num(X_interp)

    mask_nan = np.isnan(X)
    X_masked = np.ma.masked_array(X, mask_nan)
    # X_interp = pd.DataFrame(X_masked).interpolate(limit_direction='both').values  # æ­¤æ—¶ä¸ä¼šæŠ¥é”™
    X_interp = pd.DataFrame(X_masked).interpolate(limit_direction='both').values
    X = np.nan_to_num(X_interp)
    # ------------------------------
    # 4. æ ‡å‡†åŒ–
    # ------------------------------
    if scaler_type == 'minmax':
        scaler = MinMaxScaler()
    else:
        scaler = StandardScaler()

    # T, P = X.shape  # T ç°åœ¨æ˜¯ T_total (19200), P æ˜¯ 20
    # X_scaled = scaler.fit_transform(X.reshape(-1, 1)).reshape(T, P)
    # X = X_scaled
    T, P_effective = X.shape  # T æ˜¯ 19200, P_effective ç°åœ¨æ˜¯ 20
    X_scaled = scaler.fit_transform(X.reshape(-1, 1)).reshape(T, P_effective)
    X = X_scaled

    # ------------------------------
    # 5. åŠ è½½ graph.npy (P, P)
    # ------------------------------
    graph_file = os.path.join(data_path, "graph.npy")
    if not os.path.exists(graph_file):
        raise FileNotFoundError(f"graph.npy not found in {data_path}")

    mask = np.load(graph_file, allow_pickle=True)
    print(mask.shape)  # æ­¤æ—¶åº”è¯¥æ˜¯ (20, 20)

    # å¯é€‰è£å‰ª (è¿™é‡Œ n_max åº”è¯¥å’Œ X çš„ç»´åº¦ P_effective ç›¸åŒ)
    if n_max is not None and mask.shape[0] > n_max:
        mask = mask[:n_max, :n_max]

    # ç¡®ä¿ mask å’Œ X çš„ç»´åº¦åŒ¹é…
    if mask.shape[0] != P_effective:
        raise ValueError(
            f"Feature dimension P ({P_effective}) and GC mask dimension ({mask.shape[0]}) mismatch after preprocessing.")

    # ------------------------------
    # 6 & 7. æ„é€ åºåˆ—å’Œåå‘åºåˆ— (ä¿æŒä¸å˜ï¼Œä½¿ç”¨ T å’Œ P_effective)
    # ------------------------------
    test_x = X[:T - 1]   # X[:T - 1]
    test_y = X[1:T]    #  X[1:T]

    input_seq = torch.tensor(test_x, dtype=torch.float32).unsqueeze(0).to(device)  # (1, T-1, 20)
    target_seq = torch.tensor(test_y, dtype=torch.float32).to(device)  # (T-1, 20)

    X_rev = np.ascontiguousarray(X[::-1])
    rev_x = X_rev[:T - 1]
    rev_y = X_rev[1:T]

    reversed_input_seq = torch.tensor(rev_x, dtype=torch.float32).unsqueeze(0).to(device)
    reversed_target_seq = torch.tensor(rev_y, dtype=torch.float32).to(device)

    return input_seq, target_seq, reversed_input_seq, reversed_target_seq, mask, X

    # """
    # # 5. åŠ è½½ graph.npy (P, P)
    # # ------------------------------
    # graph_file = os.path.join(data_path, "graph.npy")
    #
    # if not os.path.exists(graph_file):
    #     raise FileNotFoundError(f"graph.npy not found in {data_path}")
    #
    # mask = np.load(graph_file, allow_pickle=True)
    # print(mask.shape)
    # # å¯é€‰è£å‰ª
    # if n_max is not None and mask.shape[0] > n_max:
    #     mask = mask[:n_max, :n_max]
    #
    # # ------------------------------
    # # 6. æ„é€ åºåˆ—ï¼ˆä¸ DREAM4 å®Œå…¨ä¸€è‡´ï¼‰
    # # ------------------------------
    # test_x = X[:T - 1]   # (T-1, P)
    # test_y = X[1:T]      # (T-1, P)
    #
    # input_seq = torch.tensor(test_x, dtype=torch.float32).unsqueeze(0).to(device)  # (1, T-1, P)
    # target_seq = torch.tensor(test_y, dtype=torch.float32).to(device)              # (T-1, P)
    #
    # # ------------------------------
    # # 7. åå‘åºåˆ—
    # # ------------------------------
    # X_rev = np.ascontiguousarray(X[::-1])
    # rev_x = X_rev[:T - 1]
    # rev_y = X_rev[1:T]
    #
    # reversed_input_seq = torch.tensor(rev_x, dtype=torch.float32).unsqueeze(0).to(device)
    # reversed_target_seq = torch.tensor(rev_y, dtype=torch.float32).to(device)
    #
    # return input_seq, target_seq, reversed_input_seq, reversed_target_seq, mask, X
    # """
def infer_Grangercausalityv4_inge_plus_try_tosparse_1211_single(P, type, epoch, hidden_size, learning_rate,
                                  lambda_gc_sparse_base,
                                  lag_agg='mean',
                                  cutoff_ratio=0.2,
                                    tau=10.0,
                                  ):

    global_seed = 1
    torch.manual_seed(global_seed)
    torch.cuda.manual_seed_all(global_seed)
    np.random.seed(global_seed)

    target_device_id = 1
    if torch.cuda.is_available() and torch.cuda.device_count() > target_device_id:
        device_local = torch.device(f'cuda:{target_device_id}')

    else:
        device_local = torch.device('cpu')


    global device
    device = device_local  # ç¡®ä¿å…¨å±€æˆ–å†…éƒ¨å˜é‡ä¸€è‡´
    try:
        # å‡è®¾ load_causaltime_seq è¿”å› (1, T, P) å’Œ (T, P) çš„ torch.tensor
        # å®é™…æ•°æ®åŠ è½½é€»è¾‘å¯èƒ½éœ€è¦è°ƒæ•´ä»¥é€‚åº”ä½ çš„ CausalTime æ•°æ®é›†
        data_root = f"/home/user/wcj/KANGCI-main/realdataset/{type}/"
        input_seq, target_seq, reversed_input_seq, reversed_target_seq, GC, X_raw = \
            load_causaltime_seq(data_root, device_local, scaler_type='minmax', n_max=P)
        # print('1111')
        if isinstance(GC, np.ndarray):
            GC = GC.astype(float)
    except NameError:
        pass

    P_effective = input_seq.shape[2]

    # --- æ¨¡å‹æ„å»º ---
    model_fwd = MLP(P, hidden_size, P).to(device)

    params = list(model_fwd.parameters())
    optimizer = torch.optim.Adam([
        {'params': list(model_fwd.parameters()), 'lr': learning_rate}
    ])

    loss_fn = nn.MSELoss()

    # --- è®­ç»ƒå¾ªç¯ ---
    smooth_flag = True  # é»˜è®¤å¯ç”¨å¹³æ»‘
    best_score = -1e9
    best_auprc = -1e9
    # ------------------- æ—©åœå‚æ•° -------------------
    patience = 20  # å®¹å¿å¤šå°‘è½®æ²¡æœ‰æå‡
    min_delta = 1e-5  # æœ€å°æå‡å¹…åº¦
    counter = 0  # è®¡æ•°å™¨
    previous_auroc = -1e9  # è®°å½•ä¸Šä¸€è½®çš„ AUROC
    for i in range(epoch):
        model_fwd.train()
        optimizer.zero_grad()
        inp_fwd = input_seq
        outs_fwd_all = model_fwd(inp_fwd).squeeze(0)  # (T, P)
        # é¢„æµ‹æŸå¤±
        losses_fwd = [loss_fn(outs_fwd_all[:, j], target_seq[:, j]) for j in range(P)]
        predict_loss1 = sum(losses_fwd)  # L_fwd


        GCs_raw, L_gc = compute_gradient_gc_smooth_universal_v3_1220_103v1_san(
            model_fwd,
            input_seq,
            create_graph=True,
            lag_agg_local=lag_agg,
            freq_denoise=True,  # æˆ– True
            cutoff_ratio=cutoff_ratio,
            lambda_gc=lambda_gc_sparse_base,
            tau=tau
        )

        loss = predict_loss1 + L_gc


        # --- åå‘ä¼ æ’­ ---
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)
        optimizer.step()

        GCs_np = GCs_raw.detach().cpu().numpy()
        score1,auprc1 = compute_roc_1219(GC, GCs_np, False)
        # update best by AUPRC fusion primarily, score secondarily
        if auprc1 > best_auprc:
            best_auprc = auprc1
        if score1 > best_score:
            best_score = score1
        if score1 < previous_auroc - min_delta:
            counter += 1  # è¿ç»­ä¸‹é™ï¼Œè®¡æ•°å™¨åŠ  1
        else:
            counter = 0  # æŒ‡æ ‡æ²¡æœ‰ä¸‹é™ï¼ˆæŒå¹³æˆ–ä¸Šå‡ï¼‰ï¼Œé‡ç½®è®¡æ•°å™¨
        previous_auroc = score1


        print(f"Epoch [{i + 1}/{epoch}] loss: {loss.item():.6f} | predict_loss1: {predict_loss1.item():.6f} | "
              f"Lsparse_fwd: {L_gc.item():.6f} | "
              f"score1: {score1:.4f}  | "
              f"AUPRC_fwd: {auprc1:.4f} | ")

        if counter > 0:
            print(f"  --- No improvement counter: {counter} / {patience} ---")

        # ------------------- ç»ˆæ­¢æ¡ä»¶ (æ–°å¢) -------------------
        if counter >= patience:
            print(
                f"ğŸŒŸğŸŒŸğŸŒŸ Early stopping triggered after {i + 1} epochs! AUPRC did not improve for {patience} rounds. ğŸŒŸğŸŒŸğŸŒŸ")
            break  # é€€å‡º for i in range(epoch) å¾ªç¯
    # End training
    # return best_score, best_auprc
    return best_score, best_auprc
def grid_search(param_grid):
    results_roc = []
    results_prc = []
    param_list = list(ParameterGrid(param_grid))

    for params in param_list:
        print(f"Training with params: {params}")



        avg_score = infer_Grangercausalityv4_inge_plus_try_tosparse_1211_single(20, 'medical', 500, hidden_size=params['hidden_size'],
                                                           learning_rate=params['learning_rate'],
                                                           lambda_gc_sparse_base=params['lambda_gc_sparse_base'],cutoff_ratio=params['cutoff_ratio'],
                                                           tau=params['tau'],lag_agg=params['lag_agg'])

        results_roc.append((params, avg_score[0]))
        results_prc.append((params, avg_score[1]))

    best_params_roc = max(results_roc, key=lambda x: x[1])
    best_params_prc = max(results_prc, key=lambda x: x[1])
    print(f"Best params: {best_params_roc[0]} with avg score: {best_params_roc[1]}")
    print(f"Best params: {best_params_prc[0]} with avg score: {best_params_prc[1]}")
    return best_params_roc


if __name__ == '__main__':

    # softmax  91 90   9210 9053      new 9286 9204    0.93112
    param_grid = {
        'hidden_size': [128],  ##128   orgin256
        'learning_rate': [0.0008],  # 0.005 0.001   0.0009 best?    0.0008 auroc 0.93223
        'lambda_gc_sparse_base': [0.01],  #0.005
        'cutoff_ratio': [0.6],
        'lag_agg': ['softmax'],
        'data_path': ['/home/user/wcj/KANGCI-main/realdataset'],
        'tau': [0.1],  # æ–°å¢å‚æ•°ï¼Œç”¨äº CSDRF ç»†åŒ–

    }
    best_params = grid_search(param_grid)
# Best params: {'cutoff_ratio': 0.6, 'data_path': '/home/user/wcj/KANGCI-main/realdataset', 'hidden_size': 128, 'lag_agg': 'softmax', 'lambda_gc_sparse_base': 0.01, 'learning_rate': 0.0008, 'tau': 0.1} with avg score: 0.8920377867746289
# Best params: {'cutoff_ratio': 0.6, 'data_path': '/home/user/wcj/KANGCI-main/realdataset', 'hidden_size': 128, 'lag_agg': 'softmax', 'lambda_gc_sparse_base': 0.01, 'learning_rate': 0.0008, 'tau': 0.1} with avg score: 0.8945821090061016
